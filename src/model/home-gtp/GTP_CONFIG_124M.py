GTP_CONFIG_124M = {
"vocab_size":50257,
"context_length":1024,
"emb_dim":768,
"n_head":12,
"n_layer":12,
"drop_rate":0.1,
"qkv_bias":False
}